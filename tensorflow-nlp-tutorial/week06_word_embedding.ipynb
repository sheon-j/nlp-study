{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkKGfKlou/V/hLjpOS6lfz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheon-j/nlp-study/blob/main/tensorflow-nlp-tutorial/week06_word_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Study: Week 6 - Word Embedding\n",
        "\n",
        "[딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/book/2155) 스터디\n",
        "\n",
        "---\n",
        "\n",
        "**Contents**\n",
        "8. 사전 훈련된 워드 임베딩(Pre-trained Word Embedding)\n",
        "9. 엘모(Embeddings from Language Model, ELMo)\n",
        "10. 임베딩 벡터의 시각화(Embedding Visualization)"
      ],
      "metadata": {
        "id": "WAnpv8gvIOrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. 사전 훈련된 워드 임베딩(Pre-trained Word Embedding)\n",
        "\n",
        "* **케라스의 임베딩 층(embedding layer)** 과 **사전 훈련된 워드 임베딩(pre-trained word embedding)**  비교\n",
        "\n",
        "  1. 훈련 데이터를 케라스의 Embedding 메소드를 통해서 임베딩 층을 직접 구현하고 학습\n",
        "  2. 사전 훈련된 워드 임베딩(Word2vec, FastText, GloVe)을 통해서<br>훈련 데이터와 매칭되는 임베딩 벡터를 학습"
      ],
      "metadata": {
        "id": "hCqB384r6JbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1. 케라스 임베딩 층(Keras Embedding layer)\n",
        "\n",
        "* Embedding()\n",
        "  * 훈련 데이터의 단어들에 대해 워드 임베딩을 수행하는 도구\n",
        "  * 인공 신경망 구조 관점에서 임베딩 층(embedding layer)을 구현"
      ],
      "metadata": {
        "id": "gzjaPIIs6YoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (1) 임베딩 층은 룩업 테이블이다.\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![img](https://wikidocs.net/images/page/33793/lookup_table.PNG)\n",
        "\n",
        "</div>\n",
        "\n",
        "* 임베딩 층의 입력은 정수 인코딩 된 시퀀스\n",
        "* 단어 → 단어에 매핑된 고유한 정수 값 → 임베딩 층 통과 → 밀집 벡터 \n",
        "  * 임베딩 층은 입력 정수에 대해 밀집 벡터(dense vector)로 맵핑. 이는 임베딩 벡터라고도 함\n",
        "  * 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련\n",
        "  * 훈련 과정에서 단어는 모델이 풀고자하는 작업에 맞는 값으로 업데이트\n",
        "\n",
        "* 원-핫 벡터 vs 임베딩 \n",
        "  * 원-핫 벡터\n",
        "    * tokenizer.texts_to_sequences() 정수 인코딩 >>  to_categorical() 원 핫 인코딩\n",
        "    * 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점\n",
        "    * 벡터 표현의 공간적 낭비(희소 표현)를 해결한 것이 워드 임베딩의 밀집 표현\n",
        "\n",
        "\n",
        "\n",
        "* 케라스의 임베딩 층의 기본 구현 코드\n",
        "\n",
        "  ```python\n",
        "  vocab_size = 20000\n",
        "  output_dim = 128\n",
        "  input_length = 500\n",
        "  \n",
        "  v = Embedding(vocab_size, output_dim, input_length=input_length)\n",
        "  ```\n",
        "\n",
        "  * 임베딩 층의 인자\n",
        "    * **vocab_size =** 텍스트 데이터의 전체 단어 집합의 크기\n",
        "    * **output_dim =** 워드 임베딩 후의 임베딩 벡터의 차원\n",
        "    * **input_length =** 입력 시퀀스의 길이\n",
        "\n",
        "  * 입력층\n",
        "    * Embedding()은 (number of samples, input_length)인 정수 인코딩이 된 2D 텐서를 입력\n",
        "    * Embedding()은 워드 임베딩 작업을 수행 (number of samples, input_length, embedding word dimentionality)\n",
        "    * 3D 실수 텐서를 리턴 "
      ],
      "metadata": {
        "id": "tDq6nkfIicqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (2) 임베딩 층 사용하기\n",
        "\n",
        "* 목표: 문장의 긍, 부정을 판단하는 감성 분류 모델"
      ],
      "metadata": {
        "id": "YCGEjtgCicoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
        "# 긍정은 레이블 1, 부정은 레이블 0\n",
        "y_train = [1, 0, 0, 1, 1, 0, 1]"
      ],
      "metadata": {
        "id": "n5Sh2tEVjdEn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 케라스의 토크나이저를 사용하여 단어 집합을 만들고 그 크기를 확인\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1 # 패딩을 고려하여 +1\n",
        "print('단어 집합 :',vocab_size)"
      ],
      "metadata": {
        "id": "rRFHVr3wjg4Q",
        "outputId": "dc906208-2ade-41a8-c892-857521ea3a30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 : 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 문장에 대해서 정수 인코딩을 수행\n",
        "X_encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print('정수 인코딩 결과 :',X_encoded)"
      ],
      "metadata": {
        "id": "nJ87RNOvji1q",
        "outputId": "5631be74-bb14-479a-abc5-12997ffe395b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 결과 : [[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가장 길이가 긴 문장의 길이\n",
        "max_len = max(len(l) for l in X_encoded)\n",
        "print('최대 길이 :',max_len)"
      ],
      "metadata": {
        "id": "XLlRIvn0jize",
        "outputId": "54e01d5f-57a5-41a9-e2eb-0ad269720441",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최대 길이 : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최대 길이로 모든 샘플에 대해서 패딩을 진행\n",
        "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
        "y_train = np.array(y_train)\n",
        "print('패딩 결과 :')\n",
        "print(X_train)"
      ],
      "metadata": {
        "id": "7S24VwxajixX",
        "outputId": "f0631b3d-b385-41a2-8956-48bf6127a768",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "패딩 결과 :\n",
            "[[ 1  2  3  4]\n",
            " [ 5  6  0  0]\n",
            " [ 7  8  0  0]\n",
            " [ 9 10  0  0]\n",
            " [11 12  0  0]\n",
            " [13  0  0  0]\n",
            " [14 15  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이진 분류 모델을 설계\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "embedding_dim = 4\n",
        "\n",
        "model = Sequential()\n",
        "# 임베딩 층 사용\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(Flatten())\n",
        "# 출력층에 1개의 뉴런을 배치. 활성화 함수는 시그모이드 함수\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 손실 함수로 binary_crossentropy\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "# 학습 과정에서 현재 각 단어들의 임베딩 벡터들의 값은 출력층의 가중치와 함께 학습\n",
        "# 100 에포크 학습\n",
        "model.fit(X_train, y_train, epochs=100, verbose=1)"
      ],
      "metadata": {
        "id": "3OLC3yvfjr-q",
        "outputId": "c0a7ca78-07f2-4353-a02c-ffd1d996995d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 0.6951 - acc: 0.5714\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6939 - acc: 0.5714\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6926 - acc: 0.5714\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6913 - acc: 0.5714\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6900 - acc: 0.5714\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6887 - acc: 0.5714\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6874 - acc: 0.7143\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6862 - acc: 0.8571\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6849 - acc: 0.8571\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6836 - acc: 0.8571\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6823 - acc: 1.0000\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6810 - acc: 1.0000\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6797 - acc: 1.0000\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6784 - acc: 1.0000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6771 - acc: 1.0000\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6758 - acc: 1.0000\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6745 - acc: 1.0000\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6732 - acc: 1.0000\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6719 - acc: 1.0000\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6706 - acc: 1.0000\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6693 - acc: 1.0000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6680 - acc: 1.0000\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6666 - acc: 1.0000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6653 - acc: 1.0000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6639 - acc: 1.0000\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6626 - acc: 1.0000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6612 - acc: 1.0000\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6599 - acc: 1.0000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6585 - acc: 1.0000\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6571 - acc: 1.0000\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6557 - acc: 1.0000\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6543 - acc: 1.0000\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6529 - acc: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6515 - acc: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6501 - acc: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6487 - acc: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6473 - acc: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6458 - acc: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6444 - acc: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6429 - acc: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6415 - acc: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6400 - acc: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6385 - acc: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6370 - acc: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6355 - acc: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6340 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6325 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6310 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6295 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6279 - acc: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6263 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6248 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6232 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6216 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6200 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6184 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6168 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6152 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6135 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6119 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6102 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6086 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6069 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6052 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6035 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6018 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6001 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5984 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5966 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5949 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5931 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5914 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5896 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5878 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5860 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5842 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5824 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5806 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5787 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5769 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5750 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5732 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5713 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5694 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5675 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5657 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5637 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5618 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5599 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5580 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5561 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5541 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5522 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5502 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5482 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5463 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5443 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5423 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5403 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5383 - acc: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd8fcc13210>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2. 사전 훈련된 워드 임베딩(Pre-Trained Word Embedding) 사용하기\n",
        "\n",
        "* 이미 훈련되어져 있는 워드 임베딩을 가져와서 이를 임베딩 벡터로 사용\n",
        "  * 훈련 데이터가 적은 상황\n",
        "  * 해당 문제에 특화된 것은 아니지만 보다 많은 훈련 데이터로 이미 Word2Vec이나 GloVe 등으로 학습되어져 있는 임베딩 벡터들을 사용\n",
        "  * 성능의 개선을 가져옴\n",
        "\n",
        "* 사전 훈련된 GloVe와 Word2Vec 임베딩을 사용해서 모델을 훈련시키는 실습\n",
        "  * GloVe [다운로드](http://nlp.stanford.edu/data/glove.6B.zip)\n",
        "  * Word2Vec [다운로드](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM )"
      ],
      "metadata": {
        "id": "ZgIqcvi-6ZN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 앞서 사용한 훈련 데이터\n",
        "print(X_train)\n",
        "print(y_train)"
      ],
      "metadata": {
        "id": "a-BYAe0fj-B2",
        "outputId": "26e2012a-6b5c-47ea-94b7-5099f245fe18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  2  3  4]\n",
            " [ 5  6  0  0]\n",
            " [ 7  8  0  0]\n",
            " [ 9 10  0  0]\n",
            " [11 12  0  0]\n",
            " [13  0  0  0]\n",
            " [14 15  0  0]]\n",
            "[1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (1) 사전 훈련된 GloVe 사용하기"
      ],
      "metadata": {
        "id": "UHv-CEpnkCEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# glove.6B.zip 다운 \n",
        "from urllib.request import urlretrieve, urlopen\n",
        "import gzip\n",
        "import zipfile\n",
        "\n",
        "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
        "zf = zipfile.ZipFile('glove.6B.zip')\n",
        "zf.extractall() \n",
        "zf.close()"
      ],
      "metadata": {
        "id": "HP69LkUKkA2e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# glove.6B.zip => glove.6B.100d.txt에 있는 모든 임베딩 벡터 로드 후 확인\n",
        "embedding_dict = dict()\n",
        "\n",
        "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "\n",
        "    # 100개의 값을 가지는 array로 변환\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "\n",
        "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))"
      ],
      "metadata": {
        "id": "lMTH4a8nkG4a",
        "outputId": "ccf32027-367c-4670-eb76-859bc1c7c1f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400000개의 Embedding vector가 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의의 단어 'respectable'의 임베딩 벡터값과 크기를 출력\n",
        "print(embedding_dict['respectable'])\n",
        "print('벡터의 차원 수 :',len(embedding_dict['respectable']))"
      ],
      "metadata": {
        "id": "Bbzc59QJkI6j",
        "outputId": "4414ca05-519c-474d-ba87-a699ef0fbd24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.049773   0.19903    0.10585    0.1391    -0.32395    0.44053\n",
            "  0.3947    -0.22805   -0.25793    0.49768    0.15384   -0.08831\n",
            "  0.0782    -0.8299    -0.037788   0.16772   -0.45197   -0.17085\n",
            "  0.74756    0.98256    0.81872    0.28507    0.16178   -0.48626\n",
            " -0.006265  -0.92469   -0.30625   -0.067318  -0.046762  -0.76291\n",
            " -0.0025264 -0.018795   0.12882   -0.52457    0.3586     0.43119\n",
            " -0.89477   -0.057421  -0.53724    0.25587    0.55195    0.44698\n",
            " -0.24252    0.29946    0.25776   -0.8717     0.68426   -0.05688\n",
            " -0.1848    -0.59352   -0.11227   -0.57692   -0.013593   0.18488\n",
            " -0.32507   -0.90171    0.17672    0.075601   0.54896   -0.21488\n",
            " -0.54018   -0.45882   -0.79536    0.26331    0.18879   -0.16363\n",
            "  0.3975     0.1099     0.1164    -0.083499   0.50159    0.35802\n",
            "  0.25677    0.088546   0.42108    0.28674   -0.71285   -0.82915\n",
            "  0.15297   -0.82712    0.022112   1.067     -0.31776    0.1211\n",
            " -0.069755  -0.61327    0.27308   -0.42638   -0.085084  -0.17694\n",
            " -0.0090944  0.1109     0.62543   -0.23682   -0.44928   -0.3667\n",
            " -0.21616   -0.19187   -0.032502   0.38025  ]\n",
            "벡터의 차원 수 : 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 풀고자 하는 문제의 단어 집합 크기의 행과 100개의 열을 가지는 빈 행렬 생성\n",
        "# 이후 빈 행렬에 사전 훈련된 임베딩 값 매핑\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "print('임베딩 행렬의 크기(shape) :',np.shape(embedding_matrix))"
      ],
      "metadata": {
        "id": "h8tXGHYDkLDR",
        "outputId": "8523f01a-795b-4b15-ba9d-f5d62644be3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩 행렬의 크기(shape) : (16, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 데이터 단어-정수인코딩 확인\n",
        "print(*tokenizer.word_index.items(), sep=\"\\n\")"
      ],
      "metadata": {
        "id": "dEHvPKhokMbc",
        "outputId": "1bb845e0-fb4f-4627-b5b6-666a4c38b5f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('nice', 1)\n",
            "('great', 2)\n",
            "('best', 3)\n",
            "('amazing', 4)\n",
            "('stop', 5)\n",
            "('lies', 6)\n",
            "('pitiful', 7)\n",
            "('nerd', 8)\n",
            "('excellent', 9)\n",
            "('work', 10)\n",
            "('supreme', 11)\n",
            "('quality', 12)\n",
            "('bad', 13)\n",
            "('highly', 14)\n",
            "('respectable', 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'great' 맵핑 정수는 2\n",
        "print('단어 great의 맵핑된 정수 :',tokenizer.word_index['great'])"
      ],
      "metadata": {
        "id": "55514F_-kNd3",
        "outputId": "ac9764c2-17ea-4614-b68f-cbbd2b3feffb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 great의 맵핑된 정수 : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 훈련된 GloVe에서 'great'의 벡터값을 확인\n",
        "print(embedding_dict['great'])"
      ],
      "metadata": {
        "id": "3wGGCtJjkOI8",
        "outputId": "1b42f2f9-81e8-4ea9-dde8-9b7a0c74c657",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.013786   0.38216    0.53236    0.15261   -0.29694   -0.20558\n",
            " -0.41846   -0.58437   -0.77355   -0.87866   -0.37858   -0.18516\n",
            " -0.128     -0.20584   -0.22925   -0.42599    0.3725     0.26077\n",
            " -1.0702     0.62916   -0.091469   0.70348   -0.4973    -0.77691\n",
            "  0.66045    0.09465   -0.44893    0.018917   0.33146   -0.35022\n",
            " -0.35789    0.030313   0.22253   -0.23236   -0.19719   -0.0053125\n",
            " -0.25848    0.58081   -0.10705   -0.17845   -0.16206    0.087086\n",
            "  0.63029   -0.76649    0.51619    0.14073    1.019     -0.43136\n",
            "  0.46138   -0.43585   -0.47568    0.19226    0.36065    0.78987\n",
            "  0.088945  -2.7814    -0.15366    0.01015    1.1798     0.15168\n",
            " -0.050112   1.2626    -0.77527    0.36031    0.95761   -0.11385\n",
            "  0.28035   -0.02591    0.31246   -0.15424    0.3778    -0.13599\n",
            "  0.2946    -0.31579    0.42943    0.086969   0.019169  -0.27242\n",
            " -0.31696    0.37327    0.61997    0.13889    0.17188    0.30363\n",
            " -1.2776     0.044423  -0.52736   -0.88536   -0.19428   -0.61947\n",
            " -0.10146   -0.26301   -0.061707   0.36627   -0.95223   -0.39346\n",
            " -0.69183   -1.0426     0.28855    0.63056  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 집합의 모든 단어에 대해서 사전 훈련된 GloVe의 임베딩 벡터들을 맵핑\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
        "    vector_value = embedding_dict.get(word)\n",
        "    if vector_value is not None:\n",
        "        embedding_matrix[index] = vector_value"
      ],
      "metadata": {
        "id": "xF9OrCW0kP2L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'great'의 벡터값이 의도한 인덱스의 위치에 삽입되었는지 확인\n",
        "embedding_matrix[2]\t# GloVe에서의 'great'의 벡터값과 일치!"
      ],
      "metadata": {
        "id": "6xJtelxBkR-t",
        "outputId": "53987315-d348-4e91-a3ac-923723506e64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.013786  ,  0.38216001,  0.53236002,  0.15261   , -0.29694   ,\n",
              "       -0.20558   , -0.41846001, -0.58437002, -0.77354997, -0.87866002,\n",
              "       -0.37858   , -0.18516   , -0.12800001, -0.20584001, -0.22925   ,\n",
              "       -0.42598999,  0.3725    ,  0.26076999, -1.07019997,  0.62915999,\n",
              "       -0.091469  ,  0.70348001, -0.4973    , -0.77691001,  0.66044998,\n",
              "        0.09465   , -0.44893   ,  0.018917  ,  0.33146   , -0.35021999,\n",
              "       -0.35789001,  0.030313  ,  0.22253001, -0.23236001, -0.19719   ,\n",
              "       -0.0053125 , -0.25848001,  0.58081001, -0.10705   , -0.17845   ,\n",
              "       -0.16205999,  0.087086  ,  0.63028997, -0.76648998,  0.51618999,\n",
              "        0.14072999,  1.01900005, -0.43136001,  0.46138   , -0.43584999,\n",
              "       -0.47567999,  0.19226   ,  0.36065   ,  0.78987002,  0.088945  ,\n",
              "       -2.78139997, -0.15366   ,  0.01015   ,  1.17980003,  0.15167999,\n",
              "       -0.050112  ,  1.26259995, -0.77526999,  0.36030999,  0.95761001,\n",
              "       -0.11385   ,  0.28035   , -0.02591   ,  0.31246001, -0.15424   ,\n",
              "        0.37779999, -0.13598999,  0.29460001, -0.31579   ,  0.42943001,\n",
              "        0.086969  ,  0.019169  , -0.27241999, -0.31696001,  0.37327   ,\n",
              "        0.61997002,  0.13889   ,  0.17188001,  0.30362999, -1.27760005,\n",
              "        0.044423  , -0.52736002, -0.88536   , -0.19428   , -0.61947   ,\n",
              "       -0.10146   , -0.26301   , -0.061707  ,  0.36627001, -0.95222998,\n",
              "       -0.39346001, -0.69182998, -1.04260004,  0.28854999,  0.63055998])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding layer에 embedding_matrix를 초기값으로 설정\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "output_dim = 100\t# 사전 훈련된 워드 임베딩은 100차원의 값\n",
        "\n",
        "model = Sequential()\n",
        "e = Embedding(\n",
        "\t\tvocab_size, output_dim, \n",
        "\t\tweights=[embedding_matrix], # 사전훈련된 워드 임베딩을 가중치로\n",
        "\t\tinput_length=max_len,\n",
        "\t\ttrainable=False\t\t\t\t\t# 사전훈련 워드 임베딩을 사용하므로 추가 훈련은 False\n",
        ")\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=1)"
      ],
      "metadata": {
        "id": "4b2ivf0AkSus",
        "outputId": "09386388-22cb-4cac-8cc0-d3ddcb71827a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 765ms/step - loss: 0.8275 - acc: 0.5714\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8001 - acc: 0.5714\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7735 - acc: 0.5714\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7477 - acc: 0.5714\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7228 - acc: 0.5714\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6987 - acc: 0.5714\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6756 - acc: 0.5714\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6534 - acc: 0.7143\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6321 - acc: 0.8571\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6117 - acc: 0.8571\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5923 - acc: 1.0000\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5737 - acc: 1.0000\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5561 - acc: 1.0000\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5393 - acc: 1.0000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5233 - acc: 1.0000\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5081 - acc: 1.0000\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4937 - acc: 1.0000\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4800 - acc: 1.0000\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4669 - acc: 1.0000\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4545 - acc: 1.0000\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4426 - acc: 1.0000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4312 - acc: 1.0000\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4204 - acc: 1.0000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4100 - acc: 1.0000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4000 - acc: 1.0000\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3904 - acc: 1.0000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3812 - acc: 1.0000\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3723 - acc: 1.0000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3638 - acc: 1.0000\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3555 - acc: 1.0000\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3475 - acc: 1.0000\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3398 - acc: 1.0000\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3323 - acc: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3251 - acc: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3181 - acc: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3113 - acc: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3047 - acc: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2983 - acc: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2921 - acc: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2861 - acc: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2802 - acc: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2745 - acc: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2690 - acc: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2636 - acc: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2584 - acc: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2534 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2484 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2436 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2390 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2345 - acc: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2301 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2258 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2216 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2176 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2136 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2098 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2061 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2024 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1989 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1955 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1921 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1888 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1857 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1826 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1795 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1766 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1737 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1709 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1682 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1656 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1630 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1604 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1580 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1556 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1532 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1509 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1487 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1465 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1443 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1422 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1402 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1382 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1362 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1343 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1325 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1306 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1288 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1271 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1254 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1237 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1221 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1205 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1189 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1173 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1158 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1143 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1129 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1115 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1101 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1087 - acc: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd8f4bcdb10>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 참고: 사전 훈련된 GloVe 임베딩 [예제](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)"
      ],
      "metadata": {
        "id": "Vf2ZF0bGkWXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (2) 사전 훈련된 Word2Vec 사용하기"
      ],
      "metadata": {
        "id": "L3AH21rnkY_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "# 구글의 사전 훈련된 Word2Vec 모델을 로드\n",
        "urlretrieve(\n",
        "\t\t\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\",\n",
        "\t\tfilename=\"GoogleNews-vectors-negative300.bin.gz\"\n",
        ")\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "# 모델의 크기 확인: 사전 훈련된 워드 임베딩은 300차원\n",
        "print('모델의 크기(shape) :',word2vec_model.vectors.shape)"
      ],
      "metadata": {
        "id": "zo5V8tyhkdqr",
        "outputId": "401675ac-c140-41fa-bb3d-b26647cec3c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델의 크기(shape) : (3000000, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 풀고자 하는 문제의 단어 집합 크기의 행과 300개의 열을 가지는 빈 행렬 생성\n",
        "# 이후 빈 행렬에 사전 훈련된 임베딩 값 매핑\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "print('임베딩 행렬의 크기(shape) :',np.shape(embedding_matrix))"
      ],
      "metadata": {
        "id": "j1fnDEL0kdoC",
        "outputId": "52fa4343-24cd-45ed-fe41-9cda58a176c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩 행렬의 크기(shape) : (16, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word2vec_model 특정 단어를 입력 => 해당 단어의 임베딩 벡터를 리턴\n",
        "def get_vector(word):\n",
        "    if word in word2vec_model:\n",
        "        return word2vec_model[word]\n",
        "    else:\n",
        "    \t\t# 특정 단어의 임베딩 벡터가 없다면 None 리턴\n",
        "        return None"
      ],
      "metadata": {
        "id": "1O8PCm4akdg7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 16개의 단어에 사전 훈련된 word2vec 임베딩 벡터 매핑\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
        "    vector_value = get_vector(word)\n",
        "    if vector_value is not None:\n",
        "        embedding_matrix[index] = vector_value"
      ],
      "metadata": {
        "id": "Ia6WpcABkdeB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 16개 단어의 임베딩 행렬이 완성\n",
        "print(word2vec_model['nice']) "
      ],
      "metadata": {
        "id": "yKhrDzGxkdER",
        "outputId": "6f10a2e0-d515-44af-dcab-2753574587e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.15820312  0.10595703 -0.18945312  0.38671875  0.08349609 -0.26757812\n",
            "  0.08349609  0.11328125 -0.10400391  0.17871094 -0.12353516 -0.22265625\n",
            " -0.01806641 -0.25390625  0.13183594  0.0859375   0.16113281  0.11083984\n",
            " -0.11083984 -0.0859375   0.0267334   0.34570312  0.15136719 -0.00415039\n",
            "  0.10498047  0.04907227 -0.06982422  0.08642578  0.03198242 -0.02844238\n",
            " -0.15722656  0.11865234  0.36132812  0.00173187  0.05297852 -0.234375\n",
            "  0.11767578  0.08642578 -0.01123047  0.25976562  0.28515625 -0.11669922\n",
            "  0.38476562  0.07275391  0.01147461  0.03466797  0.18164062 -0.03955078\n",
            "  0.04199219  0.01013184 -0.06054688  0.09765625  0.06689453  0.14648438\n",
            " -0.12011719  0.08447266 -0.06152344  0.06347656  0.3046875  -0.35546875\n",
            " -0.2890625   0.19628906 -0.33203125 -0.07128906  0.12792969  0.09619141\n",
            " -0.12158203 -0.08691406 -0.12890625  0.27734375  0.265625    0.1796875\n",
            "  0.12695312  0.06298828 -0.34375    -0.05908203  0.0456543   0.171875\n",
            "  0.08935547  0.14648438 -0.04638672 -0.00842285 -0.0279541   0.234375\n",
            " -0.07470703 -0.13574219  0.00378418  0.19433594  0.05664062 -0.05419922\n",
            "  0.06176758  0.14160156 -0.24121094  0.02539062 -0.15917969 -0.10595703\n",
            "  0.11865234  0.24707031 -0.13574219 -0.20410156 -0.30078125  0.07910156\n",
            " -0.04394531  0.02026367 -0.05786133  0.2109375   0.13574219  0.08349609\n",
            " -0.0098877  -0.10546875 -0.08105469  0.03735352 -0.10351562 -0.10205078\n",
            "  0.23925781 -0.21875     0.05151367  0.06738281  0.07617188  0.04638672\n",
            "  0.03198242 -0.07275391  0.14550781  0.04858398 -0.05664062 -0.07470703\n",
            " -0.0030365  -0.09277344 -0.11083984 -0.03320312 -0.15234375 -0.12207031\n",
            "  0.09814453  0.375       0.00454712 -0.10009766  0.02734375  0.30078125\n",
            " -0.0390625   0.30078125 -0.04541016 -0.00424194  0.13671875 -0.18945312\n",
            " -0.21777344  0.12695312 -0.02746582 -0.18164062  0.08984375 -0.23339844\n",
            "  0.203125    0.2734375  -0.26953125  0.15332031 -0.20703125 -0.01153564\n",
            "  0.12451172  0.05395508 -0.23535156 -0.01409912 -0.09765625  0.20800781\n",
            "  0.19335938  0.14746094  0.28710938 -0.23046875  0.01965332 -0.09619141\n",
            " -0.0703125  -0.04174805 -0.17578125  0.0007019   0.10546875  0.10351562\n",
            "  0.02478027  0.35742188  0.17382812 -0.09570312 -0.18359375  0.23242188\n",
            " -0.14453125 -0.20410156 -0.01867676  0.06640625 -0.2265625  -0.00582886\n",
            " -0.08642578  0.02416992 -0.07324219 -0.29882812 -0.15625     0.07666016\n",
            "  0.19628906 -0.20410156  0.09863281 -0.01672363 -0.18652344 -0.12353516\n",
            " -0.16015625 -0.10058594  0.21777344  0.09375    -0.10058594 -0.03637695\n",
            "  0.15136719 -0.02526855 -0.23730469  0.03417969 -0.00604248  0.15625\n",
            " -0.14257812  0.18066406 -0.35351562  0.25        0.13085938 -0.04296875\n",
            "  0.17089844  0.20507812  0.00680542 -0.08251953 -0.06738281  0.22167969\n",
            " -0.16308594 -0.16699219 -0.02087402  0.11035156  0.06054688 -0.04223633\n",
            " -0.17285156  0.05029297 -0.19824219  0.01495361  0.06542969  0.03271484\n",
            "  0.14453125 -0.08691406 -0.11035156 -0.1484375   0.09667969  0.22363281\n",
            "  0.23535156  0.08398438  0.18164062 -0.10595703 -0.04296875  0.11572266\n",
            " -0.00153351  0.0534668  -0.1328125  -0.33203125 -0.08251953  0.30664062\n",
            "  0.22363281  0.27929688  0.09082031 -0.18066406 -0.00613403 -0.09423828\n",
            " -0.21289062  0.01965332 -0.08105469 -0.06689453 -0.31835938 -0.08447266\n",
            "  0.13574219  0.0625      0.07080078 -0.14257812 -0.11279297  0.01452637\n",
            " -0.06689453  0.03881836  0.19433594  0.09521484  0.11376953 -0.12451172\n",
            "  0.13769531 -0.18847656 -0.05224609  0.15820312  0.09863281 -0.04370117\n",
            " -0.06054688  0.21679688  0.04077148 -0.14648438 -0.18945312 -0.25195312\n",
            " -0.16894531 -0.08642578 -0.08544922  0.18945312 -0.14648438  0.13476562\n",
            " -0.04077148  0.03271484  0.08935547 -0.26757812  0.00836182 -0.21386719]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 단어 집합의 nice 정수 인코딩 확인\n",
        "print('단어 nice의 맵핑된 정수 :', tokenizer.word_index['nice'])"
      ],
      "metadata": {
        "id": "dBX_M40AkdJO",
        "outputId": "9d73b18b-df53-467a-d199-1d41e1ec6e40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 nice의 맵핑된 정수 : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_matrinx의 1번 인덱스('nice') 확인\n",
        "print(embedding_matrix[1])"
      ],
      "metadata": {
        "id": "I13_P8xOkdGb",
        "outputId": "8ca24ea6-04a4-41bb-e251-667340c2fe1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.15820312  0.10595703 -0.18945312  0.38671875  0.08349609 -0.26757812\n",
            "  0.08349609  0.11328125 -0.10400391  0.17871094 -0.12353516 -0.22265625\n",
            " -0.01806641 -0.25390625  0.13183594  0.0859375   0.16113281  0.11083984\n",
            " -0.11083984 -0.0859375   0.0267334   0.34570312  0.15136719 -0.00415039\n",
            "  0.10498047  0.04907227 -0.06982422  0.08642578  0.03198242 -0.02844238\n",
            " -0.15722656  0.11865234  0.36132812  0.00173187  0.05297852 -0.234375\n",
            "  0.11767578  0.08642578 -0.01123047  0.25976562  0.28515625 -0.11669922\n",
            "  0.38476562  0.07275391  0.01147461  0.03466797  0.18164062 -0.03955078\n",
            "  0.04199219  0.01013184 -0.06054688  0.09765625  0.06689453  0.14648438\n",
            " -0.12011719  0.08447266 -0.06152344  0.06347656  0.3046875  -0.35546875\n",
            " -0.2890625   0.19628906 -0.33203125 -0.07128906  0.12792969  0.09619141\n",
            " -0.12158203 -0.08691406 -0.12890625  0.27734375  0.265625    0.1796875\n",
            "  0.12695312  0.06298828 -0.34375    -0.05908203  0.0456543   0.171875\n",
            "  0.08935547  0.14648438 -0.04638672 -0.00842285 -0.0279541   0.234375\n",
            " -0.07470703 -0.13574219  0.00378418  0.19433594  0.05664062 -0.05419922\n",
            "  0.06176758  0.14160156 -0.24121094  0.02539062 -0.15917969 -0.10595703\n",
            "  0.11865234  0.24707031 -0.13574219 -0.20410156 -0.30078125  0.07910156\n",
            " -0.04394531  0.02026367 -0.05786133  0.2109375   0.13574219  0.08349609\n",
            " -0.0098877  -0.10546875 -0.08105469  0.03735352 -0.10351562 -0.10205078\n",
            "  0.23925781 -0.21875     0.05151367  0.06738281  0.07617188  0.04638672\n",
            "  0.03198242 -0.07275391  0.14550781  0.04858398 -0.05664062 -0.07470703\n",
            " -0.0030365  -0.09277344 -0.11083984 -0.03320312 -0.15234375 -0.12207031\n",
            "  0.09814453  0.375       0.00454712 -0.10009766  0.02734375  0.30078125\n",
            " -0.0390625   0.30078125 -0.04541016 -0.00424194  0.13671875 -0.18945312\n",
            " -0.21777344  0.12695312 -0.02746582 -0.18164062  0.08984375 -0.23339844\n",
            "  0.203125    0.2734375  -0.26953125  0.15332031 -0.20703125 -0.01153564\n",
            "  0.12451172  0.05395508 -0.23535156 -0.01409912 -0.09765625  0.20800781\n",
            "  0.19335938  0.14746094  0.28710938 -0.23046875  0.01965332 -0.09619141\n",
            " -0.0703125  -0.04174805 -0.17578125  0.0007019   0.10546875  0.10351562\n",
            "  0.02478027  0.35742188  0.17382812 -0.09570312 -0.18359375  0.23242188\n",
            " -0.14453125 -0.20410156 -0.01867676  0.06640625 -0.2265625  -0.00582886\n",
            " -0.08642578  0.02416992 -0.07324219 -0.29882812 -0.15625     0.07666016\n",
            "  0.19628906 -0.20410156  0.09863281 -0.01672363 -0.18652344 -0.12353516\n",
            " -0.16015625 -0.10058594  0.21777344  0.09375    -0.10058594 -0.03637695\n",
            "  0.15136719 -0.02526855 -0.23730469  0.03417969 -0.00604248  0.15625\n",
            " -0.14257812  0.18066406 -0.35351562  0.25        0.13085938 -0.04296875\n",
            "  0.17089844  0.20507812  0.00680542 -0.08251953 -0.06738281  0.22167969\n",
            " -0.16308594 -0.16699219 -0.02087402  0.11035156  0.06054688 -0.04223633\n",
            " -0.17285156  0.05029297 -0.19824219  0.01495361  0.06542969  0.03271484\n",
            "  0.14453125 -0.08691406 -0.11035156 -0.1484375   0.09667969  0.22363281\n",
            "  0.23535156  0.08398438  0.18164062 -0.10595703 -0.04296875  0.11572266\n",
            " -0.00153351  0.0534668  -0.1328125  -0.33203125 -0.08251953  0.30664062\n",
            "  0.22363281  0.27929688  0.09082031 -0.18066406 -0.00613403 -0.09423828\n",
            " -0.21289062  0.01965332 -0.08105469 -0.06689453 -0.31835938 -0.08447266\n",
            "  0.13574219  0.0625      0.07080078 -0.14257812 -0.11279297  0.01452637\n",
            " -0.06689453  0.03881836  0.19433594  0.09521484  0.11376953 -0.12451172\n",
            "  0.13769531 -0.18847656 -0.05224609  0.15820312  0.09863281 -0.04370117\n",
            " -0.06054688  0.21679688  0.04077148 -0.14648438 -0.18945312 -0.25195312\n",
            " -0.16894531 -0.08642578 -0.08544922  0.18945312 -0.14648438  0.13476562\n",
            " -0.04077148  0.03271484  0.08935547 -0.26757812  0.00836182 -0.21386719]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding에 사전 훈련된 embedding_matrix를 입력으로 넣어주고 모델을 학습\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, Input\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(max_len,), dtype='int32'))\n",
        "e = Embedding(\n",
        "    vocab_size, 300, \n",
        "    input_length=max_len, \n",
        "    weights=[embedding_matrix], \n",
        "    trainable=False\n",
        ")\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=1)"
      ],
      "metadata": {
        "id": "fCk2rK_LklsG",
        "outputId": "4d68f35f-faca-4fa0-dfac-be6f5c71826d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 417ms/step - loss: 0.6495 - acc: 0.5714\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6327 - acc: 0.5714\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6164 - acc: 0.7143\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6007 - acc: 0.7143\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5853 - acc: 0.7143\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5705 - acc: 1.0000\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5561 - acc: 1.0000\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5422 - acc: 1.0000\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5287 - acc: 1.0000\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5157 - acc: 1.0000\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5030 - acc: 1.0000\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4908 - acc: 1.0000\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4790 - acc: 1.0000\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4675 - acc: 1.0000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4564 - acc: 1.0000\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4456 - acc: 1.0000\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4352 - acc: 1.0000\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4251 - acc: 1.0000\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4153 - acc: 1.0000\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.4058 - acc: 1.0000\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3966 - acc: 1.0000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3876 - acc: 1.0000\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3790 - acc: 1.0000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3706 - acc: 1.0000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3624 - acc: 1.0000\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3545 - acc: 1.0000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3468 - acc: 1.0000\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3394 - acc: 1.0000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3321 - acc: 1.0000\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3251 - acc: 1.0000\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3183 - acc: 1.0000\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3116 - acc: 1.0000\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3052 - acc: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2990 - acc: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2929 - acc: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2870 - acc: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2813 - acc: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2757 - acc: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2703 - acc: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2651 - acc: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2599 - acc: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2550 - acc: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2501 - acc: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2454 - acc: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2409 - acc: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2364 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2321 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2279 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2238 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2198 - acc: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2159 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2121 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2084 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2048 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2014 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1979 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1946 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1914 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1882 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1851 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1821 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1792 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1763 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1736 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1708 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1682 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1656 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1631 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1606 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1582 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1558 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1535 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1512 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1490 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1469 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1448 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1427 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1407 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1387 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1368 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1349 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1331 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1313 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1295 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1278 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1261 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1244 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1228 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1212 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1196 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1181 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1166 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1152 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1137 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1123 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1109 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1096 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1082 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1069 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1057 - acc: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd8b5597e50>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 참고: 사전 훈련된 워드 임베딩으로 의도 분류 실습 [예제](https://wikidocs.net/86083)"
      ],
      "metadata": {
        "id": "FQsqjm-0knVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. 엘모(Embeddings from Language Model, ELMo)\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![img](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSG5NF4QF_GKrEXKrGoonSYYLbNYs0RgV1VCQ&usqp=CAU)\n",
        "</div>\n",
        "\n",
        "* ELMo(Embeddings from Language Model)\n",
        "\n",
        "  * 참고: ELMo 논문 [링크](https://aclweb.org/anthology/N18-1202)\n",
        "  * 2018년에 제안된 새로운 워드 임베딩 방법론\n",
        "  * Embeddings from Language Mode: 언어 모델로 하는 임베딩\n",
        "  * **사전 훈련된 언어 모델(Pre-trained language model)**을 사용\n",
        "  ```\n",
        "  현재 텐서플로우 2.0에서는 TF-Hub의 ELMo를 사용할 수 없습니다. \n",
        "  사용하려면 텐서플로우 버전을 1버전으로 낮추어야 합니다. \n",
        "  Colab에서는 손쉽게 텐서플로우 버전을 1버전으로 설정할 수 있습니다.\n",
        "  Colab에서 실습하시는 것을 권장드립니다. \n",
        "  ```"
      ],
      "metadata": {
        "id": "IlGD7Fwk6ZCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1. ELMo(Embeddings from Language Model)\n",
        "\n",
        "* 동음이의어\n",
        "  * Bank Account(은행 계좌) vs River Bank(강둑)\n",
        "  * Word2Vec이나 GloVe 등으로 표현된 임베딩 벡터들은 이를 제대로 반영하지 못함\n",
        "    * Bank는 전혀 다른 의미임에도 불구하고 두 가지 상황 모두에서 같은 벡터가 사용 ([0.2 0.8 -1.2])\n",
        "* **문맥을 반영한 워드 임베딩(Contextualized Word Embedding)**\n",
        "  * 문맥에 따라서 다르게 워드 임베딩하므로 자연어 처리의 성능 향상\n",
        "  * 워드 임베딩 시 문맥을 고려해서 임베딩을 하겠다는 아이디어"
      ],
      "metadata": {
        "id": "fEMvX4T66ZLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2. biLM(Bidirectional Language Model)의 사전 훈련\n",
        "\n",
        "* 깊은 단방향 RNN\n",
        "\n",
        "  ![img](https://wikidocs.net/images/page/33930/deepbilm.PNG)\n",
        "\n",
        "  * 은닉층이 2개인 일반적인 단방향 RNN 언어 모델의 언어 모델링\n",
        "  * RNN의 은닉 상태의 ht의 값이 문장의 문맥 정보를 점차적으로 반영\n",
        "  * ELMo는 순방향 RNN + 역방향 RNN 활용. **biLM(Bidirectional Language Model)**\n",
        "\n",
        "* ELMo의 biLM\n",
        "\n",
        "  ![img](https://wikidocs.net/images/page/33930/forwardbackwordlm2.PNG)\n",
        "\n",
        "  * 기본적으로 다층 구조(Multi-layer)를 전제\n",
        "\n",
        "  * biLM의 입력 벡터는 합성곱 신경망 방식의 **문자 임베딩**을 거친 단어 벡터 (임베딩 층 아님)\n",
        "\n",
        "    * 문자 임베딩(character embedding)은 NLP를 위한 합성곱 신경망 챕터에서 다룸\n",
        "    * 문자 임베딩은 임베딩층, Word2Vec 와는 다른 단어 벡터를 얻는 방식\n",
        "    * 문맥과 상관없이 dog란 단어와 doggy란 단어의 연관성을 찾아낼 수 있음 (OOV에도 견고) \n",
        "\n",
        "  * **양방향 RNN**과 ELMo에서의 **biLM**은 다름\n",
        "\n",
        "    * 양방향 RNN은 순방향 은닉 상태와 역방향의 은닉 상태를 연결(concatenate)하여 다음층의 입력으로 사용\n",
        "\n",
        "      ![img](https://wikidocs.net/images/page/22886/rnn_image5_ver2.PNG)\n",
        "\n",
        "    * biLM의 순방향 언어모델과 역방향 언어모델이라는 두 개의 언어 모델을 별개의 모델로 보고 학습"
      ],
      "metadata": {
        "id": "OGa0KARl6ZJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3. biLM의 활용\n",
        "\n",
        "* 사전 훈련 모델 biLM(ELMo)을 통해 입력으로부터 단어를 임베딩하는 과정\n",
        "  ![img](https://wikidocs.net/images/page/33930/playwordvector.PNG)\n",
        "\n",
        "  * 해당 시점(time step)의 BiLM의 각 층 출력값을 로드\n",
        "    * 출력값 이란 첫번째는 임베딩 층을, 나머지는 각 층의 은닉 상태\n",
        "    * 각 층의 출력값이 가진 정보는 전부 서로 다른 종류의 정보. 이를 모두 활용\n",
        "  * 순방향 언어 모델과 역방향 언어 모델의 각 층의 출력값을 연결(concatenate)\n",
        "  * 추가 작업을 진행\n",
        "    * 각 층의 연결된 출력값에 가중치를 곱한 값을 합하여 스칼라 매개변수를 곱함"
      ],
      "metadata": {
        "id": "1gDltf2G6ZHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) 각 층의 출력값을 연결(concatenate)한다.**\n",
        "\n",
        "![img](https://wikidocs.net/images/page/33930/concatenate.PNG)\n",
        "\n",
        "**2) 각 층의 출력값 별로 가중치를 준다.**\n",
        "\n",
        "![img](https://wikidocs.net/images/page/33930/weight.PNG)\n",
        "\n",
        "* 이 가중치를 여기서는 s1, s2, s3\n",
        "\n",
        "**3) 각 층의 출력값을 모두 더한다.**\n",
        "\n",
        "![img](https://wikidocs.net/images/page/33930/weightedsum.PNG)\n",
        "\n",
        "* 2)번과 3)번의 단계를 요약하여 가중합(Weighted Sum)\n",
        "\n",
        "**4) 벡터의 크기를 결정하는 스칼라 매개변수를 곱한다.**\n",
        "\n",
        "![img](https://wikidocs.net/images/page/33930/scalarparameter.PNG)\n",
        "\n",
        "* 스칼라 매개변수를 γ (감마)로 표현\n",
        "\n",
        "\n",
        "\n",
        "* ELMo 표현(representation)\n",
        "\n",
        "  * 완성된 벡터를 ELMo 표현이라고 함\n",
        "  * ELMo 표현을 얻기 위한 과정 \n",
        "  * ELMo를 입력으로 사용하고 수행하고 싶은 텍스트 분류, 질의 응답 시스템 등의 자연어 처리 작업을 진행\n",
        "\n",
        "* ELMo를 활용한 모델링\n",
        "\n",
        "  ![img](https://wikidocs.net/images/page/33930/elmorepresentation.PNG)\n",
        "\n",
        "  * ELMo 표현을 기존의 임베딩 벡터와 함께 사용\n",
        "  * 텍스트 분류 작업을 가정\n",
        "    * GloVe와 같은 기존의 방법론을 사용한 임베딩 벡터를 준비\n",
        "    * 여기에 ELMo 표현을 GloVe 임베딩 벡터와 연결(concatenate)해서 입력으로 사용\n",
        "    * 이때 biLM의 가중치는 고정, 위에서 사용한 s1, s2, s3와 γ는 훈련 과정에서 학습"
      ],
      "metadata": {
        "id": "7Slrfe18l3_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.4. ELMo 표현을 사용해서 스팸 메일 분류하기\n",
        "* 사전 훈련된 모델로부터 ELMo 표현을 사용해보는 정도로 예제"
      ],
      "metadata": {
        "id": "hlKYrySo6ZEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 시작 전에 텐서플로우 버전을 1버전으로 설정 (colab) => 런타임 재시작\n",
        "%tensorflow_version 1.x "
      ],
      "metadata": {
        "id": "lnv1GMNqmhO2",
        "outputId": "1448a3e4-a8db-4f8f-e59c-93419248e634",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 텐서플로우 허브로부터 다양한 사전 훈련된 모델(Pre-tained Model) 사용 가능\n",
        "!pip install tensorflow-hub"
      ],
      "metadata": {
        "id": "u6Za3017mhM3",
        "outputId": "339810b5-1576-4240-f75f-29a02ffaf511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.21.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "hd9Pl_qLmhKy",
        "outputId": "12196e73-955b-4ac1-de65-f6e9ded0e496",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "elmo = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=True)\n",
        "# 텐서플로우 허브로부터 ELMo를 다운로드\n",
        "\n",
        "sess = tf.Session()\n",
        "K.set_session(sess)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(tf.tables_initializer())"
      ],
      "metadata": {
        "id": "be3SxtGlmhJB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스팸 메일 분류 데이터를 다운(kaggle sms-spam-collection-dataset)\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv\", filename=\"spam.csv\")\n",
        "data = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "-cikeDt6mhHA",
        "outputId": "7386ca67-2b53-4abe-c292-d5067d681438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     v1                                                 v2 Unnamed: 2  \\\n",
              "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
              "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
              "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
              "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
              "\n",
              "  Unnamed: 3 Unnamed: 4  \n",
              "0        NaN        NaN  \n",
              "1        NaN        NaN  \n",
              "2        NaN        NaN  \n",
              "3        NaN        NaN  \n",
              "4        NaN        NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-271eb08d-b9d5-46ea-b163-c865ac542983\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-271eb08d-b9d5-46ea-b163-c865ac542983')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-271eb08d-b9d5-46ea-b163-c865ac542983 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-271eb08d-b9d5-46ea-b163-c865ac542983');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 피쳐 v2열과 타겟 v1\n",
        "data['v1'] = data['v1'].replace(['ham','spam'],[0,1])\n",
        "y_data = list(data['v1'])\n",
        "X_data = list(data['v2'])"
      ],
      "metadata": {
        "id": "GKf_Lhg9mhED"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(*X_data[:5], sep=\"\\n\")\n",
        "print(y_data[:5])"
      ],
      "metadata": {
        "id": "UD4Wd3bAmhBZ",
        "outputId": "a55be2ae-4158-44bd-cb4f-9c548f831523",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "Ok lar... Joking wif u oni...\n",
            "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "U dun say so early hor... U c already then say...\n",
            "Nah I don't think he goes to usf, he lives around here though\n",
            "[0, 0, 1, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터, 테스트 데이터 split\n",
        "print(len(X_data))\n",
        "n_of_train = int(len(X_data) * 0.8)\n",
        "n_of_test = int(len(X_data) - n_of_train)\n",
        "print(n_of_train)\n",
        "print(n_of_test)\n",
        "\n",
        "X_train = np.asarray(X_data[:n_of_train]) #X_data 데이터 중에서 앞의 4457개의 데이터만 저장\n",
        "y_train = np.asarray(y_data[:n_of_train]) #y_data 데이터 중에서 앞의 4457개의 데이터만 저장\n",
        "X_test = np.asarray(X_data[n_of_train:]) #X_data 데이터 중에서 뒤의 1115개의 데이터만 저장\n",
        "y_test = np.asarray(y_data[n_of_train:]) #y_data 데이터 중에서 뒤의 1115개의 데이터만 저장"
      ],
      "metadata": {
        "id": "XIkqxVIfm_2Z",
        "outputId": "3711072c-9fec-46bf-e117-4c56995c6413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5572\n",
            "4457\n",
            "1115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ELMo와 설계한 모델을 연결하는 작업\n",
        "# TF 허브에서 가져온 모델을 케라스에서 사용할 수 있도록 변환해주는 작업\n",
        "def ELMoEmbedding(x):\n",
        "    # 데이터의 이동이 케라스 → 텐서플로우 → 케라스가 되도록 하는 함수\n",
        "    return elmo(tf.squeeze(tf.cast(x, tf.string)), as_dict=True, signature=\"default\")[\"default\"]"
      ],
      "metadata": {
        "id": "HD3nr5UCnI3F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 설계\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Lambda, Input\n",
        "\n",
        "input_text = Input(shape=(1,), dtype=tf.string)\n",
        "# ELMo를 이용한 임베딩 층\n",
        "embedding_layer = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\n",
        "# 256개의 뉴런이 있는 은닉층\n",
        "hidden_layer = Dense(256, activation='relu')(embedding_layer)\n",
        "# 1개의 뉴런을 통해 이진 분류. 시그모이드 활성화 함수\n",
        "output_layer = Dense(1, activation='sigmoid')(hidden_layer)\n",
        "model = Model(inputs=[input_text], outputs=output_layer)\n",
        "# 손실 함수는 binary_crossentropy\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "RywoA7O1nMNe",
        "outputId": "15efc8e6-92ac-4257-8a79-189c5ef1d86b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터에서는 정확도 96%. 놀랍다!\n",
        "history = model.fit(X_train, y_train, epochs=1, batch_size=60)"
      ],
      "metadata": {
        "id": "EWhf3jJInNlb",
        "outputId": "9710c42c-f6cc-4fed-ea4b-1405057e9651",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1\n",
            "4457/4457 [==============================] - 86s 19ms/step - loss: 0.1226 - accuracy: 0.9574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1번의 에포크에서 테스트 데이터 정확도 98%. 믿을 수 없는!\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "id": "0zcmhl5-ngOZ",
        "outputId": "261d3cfe-12f6-4a2b-9652-24a84ae09b5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115/1115 [==============================] - 22s 20ms/step\n",
            "\n",
            " 테스트 정확도: 0.9767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. 임베딩 벡터의 시각화(Embedding Visualization)\n",
        "\n",
        "* 구글에서 임베딩 프로젝터(embedding projector)라는 데이터 시각화 도구를 지원\n",
        "* 학습한 임베딩 벡터들을 시각화해보는 실습\n",
        "\n",
        "* 참고: 임베딩 프로젝터 논문 [링크](https://arxiv.org/pdf/1611.05469v1.pdf )"
      ],
      "metadata": {
        "id": "4wuqZHV06Y_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.1. 워드 임베딩 모델로부터 2개의 tsv 파일 생성하기\n",
        "\n",
        "* 시각화를 위해서는 학습이 끝난 모델이 파일로 저장되어 있어야 함\n",
        "\n",
        "* 아래 커맨드를 통해 시각화에 필요한 파일들을 생성\n",
        "\n",
        "  ```python\n",
        "  # 이전 챕터에서 저장한 영어 Word2Vec 모델 사용\n",
        "  # !python -m gensim.scripts.word2vec2tensor --input 모델이름 --output 모델이름\n",
        "!python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v\n",
        "  ```\n",
        "\n",
        "* eng_w2v 외 두 개의 파일이 추가\n",
        "\n",
        "  ![img](https://wikidocs.net/images/page/50704/eng_w2v.PNG)\n",
        "\n",
        "  * eng_w2v_metadata.tsv\n",
        "  * eng_w2v_tensor.tsv \n",
        "  * 이 두 개 파일이 임베딩 벡터 시각화를 위해 사용할 파일\n"
      ],
      "metadata": {
        "id": "E_DQ2GGI65tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2. 임베딩 프로젝터를 사용하여 시각화하기\n",
        "\n",
        "1. 구글 임베딩 프로젝터에 [접속](https://projector.tensorflow.org/ )\n",
        "\n",
        "2. 좌측 상단을 보면 Load 버튼 클릭\n",
        "\n",
        "   ![img](https://wikidocs.net/images/page/50704/embedding_projector.PNG)\n",
        "\n",
        "\n",
        "\n",
        "3. 두 개의 choose file 버튼 생성\n",
        "\n",
        "   ![img](https://wikidocs.net/images/page/50704/embedding_projector2.PNG)\n",
        "\n",
        "   * Load a TSV file of vectors => Choose file => *_tensor.tsv 업로드\n",
        "   * Load a TSV file of metadata => Choose file => *_metadata.tsv 업로드\n",
        "\n",
        "4. 임베딩 프로젝터에 학습했던 워드 임베딩 모델이 시각화\n",
        "\n",
        "   ![img](https://wikidocs.net/images/page/50704/man.PNG)\n",
        "\n",
        "5. 임베딩 프로젝터의 다양한 기능 사용 가능\n",
        "\n",
        "   * 복잡한 데이터를 차원을 축소하여 시각화 할 수 있도록 도와주는 PCA, t-SNE 등을 제공\n",
        "   * 위의 그림은 'man' 이라는 단어를 선택하고, 코사인 유사도를 기준으로 가장 유사한 상위 10개 벡터들을 표시"
      ],
      "metadata": {
        "id": "2vKlVRRa6-WX"
      }
    }
  ]
}